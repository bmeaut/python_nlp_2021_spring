{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Natural Language Technologies\n",
    "\n",
    "__Laboratory 10- NLP applications, Dependency parsing__\n",
    "\n",
    "__April 22, 2021__\n",
    "\n",
    "During this laboratory you will have to implement various evaluation methods and use them to measure the performance of pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "import spacy\n",
    "from gensim.summarization import summarizer as gensim_summarizer\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "import conllu\n",
    "import os\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza.download('en')\n",
    "stanza_nlp = stanza.Pipeline('en')\n",
    "spacy_nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the UD treebanks if you do not have them already. We are going to use them for evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-3424/ud-treebanks-v2.7.tgz\"\n",
    "tgz = 'ud-treebanks-v2.7.tgz'\n",
    "directory = 'ud_treebanks'\n",
    "if not os.path.exists(directory):\n",
    "    import tarfile\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(tgz, 'wb') as ud:\n",
    "        ud.write(response.content)\n",
    "    os.mkdir(directory)\n",
    "    with tarfile.open(tgz, 'r:gz') as _tar:\n",
    "        for member in _tar:\n",
    "            if member.isdir():\n",
    "                continue\n",
    "            fname = member.name.rsplit('/',1)[1]\n",
    "            _tar.makefile(member, os.path.join(directory, fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"ud_treebanks/en_ewt-ud-train.conllu\"\n",
    "with open(data) as conll_data:\n",
    "    trees = conllu.parse(conll_data.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trees[0].serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. F-score\n",
    "\n",
    "Probably the most relevant measure we can use when we are evaluating classifiers.\n",
    "\n",
    "Implement the function below. The function takes two iterables and returns a detailed dictionary that contains the True Positive, True Negative, False Positive, Precision, Recall, F-score values for each unique class in the gold list. Additionally, the dictionary should contain the micro and macro precision, recall and F-score values as well.\n",
    "\n",
    "You can read about the F-measure [here](https://en.wikipedia.org/wiki/F-score).\n",
    "\n",
    "Help for the micro-macro averages: https://tomaxent.com/2018/04/27/Micro-and-Macro-average-of-Precision-Recall-and-F-Score/.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_dict = {\n",
    "    0: {'tp': 4, 'fp': 0, 'fn': 0, 'precision': 1.0, 'recall': 1.0, 'f': 1.0}, \n",
    "    1: {'tp': 4, 'fp': 0, 'fn': 0, 'precision': 1.0, 'recall': 1.0, 'f': 1.0}, \n",
    "    2: {'tp': 4, 'fp': 0, 'fn': 0, 'precision': 1.0, 'recall': 1.0, 'f': 1.0}, \n",
    "    'MICRO AVG': {'precision': 1.0, 'recall': 1.0, 'f': 1.0}, \n",
    "    'MACRO AVG': {'precision': 1.0, 'recall': 1.0, 'f': 1.0}\n",
    "}\n",
    "\n",
    "f_dict2 = {\n",
    "    0: {'tp': 3, 'fp': 1, 'fn': 1, 'precision': 0.75, 'recall': 0.75, 'f': 0.75},\n",
    "    1: {'tp': 3, 'fp': 1, 'fn': 1, 'precision': 0.75, 'recall': 0.75, 'f': 0.75},\n",
    "    2: {'tp': 2, 'fp': 2, 'fn': 2, 'precision': 0.5, 'recall': 0.5, 'f': 0.5},\n",
    "    'MICRO AVG': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f': 0.6666666666666666},\n",
    "    'MACRO AVG': {'precision': 0.6666666666666666, 'recall': 0.6666666666666666, 'f': 0.6666666666666666}\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(gold, predicted):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = [0, 0, 1, 1, 2, 2, 0, 1, 2, 0, 1, 2]\n",
    "pred = [0, 2, 1, 1, 2, 0, 0, 2, 1, 0, 1, 2]\n",
    "\n",
    "assert f_dict == f_score(gold, gold)\n",
    "assert f_dict2 == f_score(gold, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Evaluate a pretrained POS tagger using the example\n",
    "\n",
    "Choose an existing POS tagger (eg. stanza, spacy, nltk) and predict the POS tags of the sentence given below. Compare the results to the refference below using the f_score function above. Keep in mind, that there are different POS formats, and you should compare them accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = trees[0].metadata[\"text\"]\n",
    "upos = [token['upos'] for token in trees[0]]\n",
    "xpos = [token['xpos'] for token in trees[0]]\n",
    "\n",
    "print(f'{sentence}\\n{upos}\\n{xpos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ROUGE-N score\n",
    "\n",
    "We usually use the ROUGE score to evaluate summaries, comparing the reference summaries and the generated summaries. Write a function that gets a reference summary, a generated summary and a number N. The number represents the length of n-grams to compare. The function should return a dictionary containing the precision, recall and f-score of the ROUGE-N score. (I practice, the most important part of the ROUGE score is its recall.)\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall = \\frac{overlapping\\ ngrams}{all\\ ngrams\\ in\\ the\\ reference\\ summary}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision = \\frac{overlapping\\ ngrams}{all\\ ngrams\\ in\\ the\\ generated\\ summary}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 = 2 * \\frac{Precision * Recall}{Precision + Recall}\n",
    "\\end{equation*}\n",
    "\n",
    "You can read further about the ROUGE-N scoring method [here](https://www.aclweb.org/anthology/W04-1013.pdf).\n",
    "\n",
    "You are encouraged to implement and use the helper functions outlined below. You can use any tokenizer you'd like for this exercise.\n",
    "\n",
    "Example results of the rouge_n function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = {'precision': 0.75, 'recall': 0.6, 'f': 0.6666666666666665}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram(text, n):\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def rouge_n(reference, generated, n):\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = 'this cat is absoultely adorable today'\n",
    "generated = 'this cat is adorable today'\n",
    "assert n2 == rouge_n(reference, generated, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Evaluate a pretraied summarizer using the example\n",
    "\n",
    "Choose a summarizer (eg. gensim, huggingface) and summarize the following text (taken from the [CNN-Daily Mail dataset](https://cs.nyu.edu/~kcho/DMQA/)) and calculate the ROUGE-2 score of the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"Manchester City starlet Devante Cole, son of Andy Cole, has joined Barnsley on loan until January.\n",
    "City have also confirmed that £3m midfielder Bruno Zuculini has joined Valencia on loan for the rest of the season. \n",
    "Meanwhile Juventus and Roma remain keen on signing Matija Nastasic.\n",
    "On the move: Manchester City striker Devante Cole, son of Andy, has joined Barnsley on loan\"\"\"\n",
    "\n",
    "reference = \"\"\"Devante Cole has joined Barnsley on loan until January.\n",
    "Son of Andy Cole has impressed in the City youth ranks.\n",
    "City have also confirmed that Bruno Zuculini has joined Valencia.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dependency parse evaluation\n",
    "\n",
    "We've discussed the two methods used to evaluate dependency parsers.\n",
    "\n",
    "Reminder:\n",
    "\n",
    " - Labeled attachment score (LAS): the percentage of words that are assigned both the correct syntactic head and the correct dependency label\n",
    " - Unlabeled attachment score (UAS): the percentage of words that are assigned both the correct syntactic head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 UAS method\n",
    "\n",
    "Implement the UAS method for evaluating graphs!\n",
    "The input of the function should be two graphs, both in formatted in a simplified conll-dict format, where the keys are the indices of the tokens and the values are tuples consisting of the head and the dependency relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_conllu(tree):\n",
    "    return {token['id']: (token['head'], token['deprel']) for token in tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reference_graph = convert_conllu(trees[0])\n",
    "reference_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = {1: (0, 'root'), 2: (1, 'punct'), 3: (1, 'flat'), 4: (1, 'punct'), 5: (6, 'amod'),\n",
    "        6: (7, 'obj'), 7: (1, 'parataxis'), 8: (7, 'obj'), 9: (8, 'flat'), 10: (8, 'flat'),\n",
    "        11: (8, 'punct'), 12: (8, 'flat'), 13: (8, 'punct'), 14: (15, 'det'), 15: (8, 'appos'),\n",
    "        16: (18, 'case'), 17: (10, 'det'), 18: (7, 'obl'), 19: (8, 'case'), 20: (21, 'det'),\n",
    "        21: (18, 'obl'), 22: (23, 'case'), 23: (21, 'nmod'), 24: (21, 'punct'), 25: (28, 'case'),\n",
    "        26: (28, 'det'), 27: (28, 'amod'), 28: (8, 'obl'), 29: (1, 'punct')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uas(gold, predicted):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 LAS method\n",
    "Implement the LAS method as well, similarly to the previous evaluation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def las(gold, predicted):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 26/29 == uas(reference_graph, pred)\n",
    "assert 24/29 == las(reference_graph, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ PASSING LEVEL ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Try out the evaluation methods with stanza\n",
    "\n",
    "Evaluate the predictions of stanza on the given example! To do so, you will have to convert the output of stanza to be in the same format as the expected input of the uas and las methods. We recomend the stanza [documentation](https://stanfordnlp.github.io/stanza/tutorials.html) to be able to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanza_converter(stanza_doc):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Compare the accuracy of stanza and spacy\n",
    "\n",
    "Run the spacy dependency parser on the same input as before and evaluate the performace. To do so you will have to implement a function, that converts the output of spacy (see the [documentation](https://spacy.io/usage/linguistic-features#dependency-parse)) to the appropriate format and check the output of the las and uas methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_converter(spacy_doc):\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ================ EXTRA LEVEL ===================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
