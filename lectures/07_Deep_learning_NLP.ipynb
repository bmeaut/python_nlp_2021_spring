{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Python and Natural Language Technologies\n",
    "## Lecture 07, Deep Learning for NLP\n",
    "### March 23, 2021\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data science\n",
    "\n",
    "> The nontrivial extraction of implicit, previously unknown, and\n",
    "> potentially useful information from data.\n",
    "\n",
    "-   non-trivial\n",
    "\n",
    "-   relationship between data points\n",
    "\n",
    "-   (large) dataset\n",
    "\n",
    "-   make predictions on unknown examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples and counterexamples Convenience store statistics\n",
    "\n",
    "-   Number of customers\n",
    "\n",
    "    -   trivial information\n",
    "\n",
    "-   Last month’s income\n",
    "\n",
    "    -   trivial information\n",
    "\n",
    "-   Items most frequently bought together\n",
    "\n",
    "    -   *finding frequent itemsets*\n",
    "\n",
    "-   How many cashiers need to be open at Friday 16 pm?\n",
    "\n",
    "    -   customer queuing model\n",
    "\n",
    "    -   *time series modeling*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector Representation\n",
    "\n",
    "\n",
    "- sample  \n",
    "    - one data point - <span style=\"color: darkred\">vector</span>\n",
    "\n",
    "- feature  \n",
    "    - a property or attribute of a sample - <span style=\"color: darkred\">one element of a vector</span>\n",
    "\n",
    "    - length of the mail\n",
    "\n",
    "    -   sender\n",
    "\n",
    "    -   Does it contain the word *Rolex*?\n",
    "\n",
    "    -   Does it contain the expression *Trust fund*?\n",
    "\n",
    "- dataset  \n",
    "    - collections of all samples - <span style=\"color: darkred\">matrix</span>\n",
    "\n",
    "- label  \n",
    "    - correct *answers* for all samples in a dataset - <span style=\"color: darkred\">vector</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural networks and Deep learning\n",
    "- Neural network is inspired by the information processing methods of biological nervous systems\n",
    "- It is composed of neurons, each layer connected to the next\n",
    "- Deep learning is a neural network consisting of multiple layers\n",
    "    - the idea is not new\n",
    "    - it is returned because of the rise of the GPUs\n",
    "    - good frameworks (Pytorch, Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- it has a black-box nature\n",
    "- interpreting them is hard\n",
    "- we don't exactly know the reasoning behind a decision\n",
    "- can we trust deep learning?\n",
    "- the latest language model of **OpenAI**, **GPT-3**, has 175B trainable parameters [link](https://news.developer.nvidia.com/openai-presents-gpt-3-a-175-billion-parameters-language-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/dl/network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is a __feed forward neural network__ with two hidden layers. Each neuron contains an activation function:\n",
    "\n",
    "$$\\mathbf{h_1} = \\sigma (\\mathbf{W_1 x})$$\n",
    "$$\\mathbf{h_2} = \\sigma (\\mathbf{W_2 h_1})$$\n",
    "$$\\mathbf{y} = \\sigma (\\mathbf{W_3 h_2})$$\n",
    "\n",
    "$\\sigma$: activation function, typically non-linear such as the sigmoid\n",
    "function $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "During training, weights are learned to predict a value of a new input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![act](https://cdn-images-1.medium.com/max/1000/1*4ZEDRpFuCIpUjNgjDdT2Lg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__What is inside a neuron?__\n",
    "\n",
    "![perceptron](https://c.mql5.com/2/35/artificialneuron__1.gif)\n",
    "\n",
    "\n",
    "*image from https://www.mql5.com/en/blogs/post/724245*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML vs DL\n",
    "\n",
    "<img src=\"img/dl/ai_ml_dl.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML vs DL\n",
    "\n",
    "- Deep learning\n",
    "    - Automatic feature engineering\n",
    "    - Scalable with big data\n",
    "    - Can solve non-separable problems as well (traditional methods struggle with non-linearity)\n",
    "    - Currently most state-of-the-art methods are based on DL\n",
    "- Traditional machine Learning\n",
    "    - Feature extraction is done manually\n",
    "    - Can learn relatively well from small data (DL can’t)\n",
    "    - Scalability is worse with big data\n",
    "    - It can be enough for small tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning types \n",
    "-   **Supervised learning**\n",
    "\n",
    "    -   is a problem where for every input variable(x) there is an ouput\n",
    "        variable(y) in the training data\n",
    "\n",
    "    -   the preparation of the output variables is usually done with\n",
    "        Human resources - Labeled data\n",
    "\n",
    "-   **Unsupervised learning**\n",
    "\n",
    "    -   is a problem where only the input variable(x) is present in the\n",
    "        training data\n",
    "\n",
    "    -   still can be very useful since labeling data is very resource\n",
    "        hungry and expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning problems\n",
    "\n",
    "-   __Classification__ (supervised learning)\n",
    "\n",
    "    -   assign a label for each sample\n",
    "\n",
    "    -   labels are predefined and usually not very numerous\n",
    "\n",
    "    -   e.g. sentiment analysis\n",
    "\n",
    "-   __Regression__ (supervised learning)\n",
    "\n",
    "    -   predict a continuous variable\n",
    "\n",
    "    -   e.g. predict real estate prices, stock market based on history,\n",
    "        location, amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   __Clustering__ (unsupervised learning)\n",
    "\n",
    "    -   group samples into clusters according to a similarity measure\n",
    "     \n",
    "    -   e.g. group similar facebook comments\n",
    "    \n",
    "    -   goal: high intra-group similarity (samples in the same cluster\n",
    "        should be similar to each other), low inter-group similarity\n",
    "        (samples in different clusters shouldn’t be similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/dl/sup_unsup.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation - Binary classification\n",
    "\n",
    "<img src=\"img/dl/true_false.png\" />\n",
    "\n",
    "**Accuracy**: fraction of correctly guessed labels among all the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### __Precision, recall and F-score:__\n",
    "\n",
    "**Precision**: fraction of positive samples among those labeled positive\n",
    "$$\\text{Precision}=\\frac{tp}{tp+fp}$$\n",
    "\n",
    "**Recall**: fraction of recovered positive samples of all positive\n",
    "samples $$\\text{Recall}=\\frac{tp}{tp+fn}$$\n",
    "\n",
    "**F-score**: harmonic mean of precision and recall\n",
    "$$\\text{F-score} = 2 * \\frac{\\text{prec}  \\text{rec}}{\\text{prec} + \\text{rec}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation - regression Root-mean-square error\n",
    "\n",
    "$$\\operatorname{RMSE}=\\sqrt{\\frac{\\sum_{t=1}^n (\\hat y_t - y_t)^2}{n}},$$\n",
    "\n",
    "where $\\hat y_t$ are the predicted values, $y_t$ is the true value and\n",
    "$n$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train, Validation and Test set\n",
    "\n",
    "- __training set__\n",
    "    - part of the dataset used for training -\n",
    "\n",
    "\n",
    "- __validation dataset__\n",
    "    - part of the dataset used for cross-validation, early stopping and\n",
    "    hyperparameter tuning\n",
    "\n",
    "\n",
    "- __test set__\n",
    "    - part of the dataset used for testing trained models. Your method\n",
    "    should only be tested once on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/dl/train_test_val.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "-   How do ML/DL algorithms learn?\n",
    "\n",
    "-   **Loss function**: helps to calculate the prediction loss of our\n",
    "    network, which tells us how bad/good is our model.\n",
    "\n",
    "-   We want to **optimize** the loss/cost function.\n",
    "\n",
    "-   How?\n",
    "\n",
    "    -   **Gradient descent** helps us find the global minima of the loss\n",
    "        function\n",
    "\n",
    "    -   **Backpropagation** algorithm is used to propagate the error\n",
    "        back to the weights of the model and updates them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Important concepts in machine learning\n",
    "\n",
    "- __Cost Function__: used to measure how badly our models are performing on a data\n",
    "- __Parameters__: variables that are updated during the training\n",
    "- __Sample__: single row in our data\n",
    "- __Batch size__: the number of samples our model works throught before updating the weights\n",
    "- __Epoch__: one epoch means that each sample in the training dataset was iterated through the model\n",
    "- __Iteration__ – one update on the weights. It happens once for each batch.\n",
    "- __Hyperparameters__ – variables that don't change during training (number of epochs, batch size, learning rate, etc..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Gradient descent__: used to find the global optimum in the cost function\n",
    "\n",
    "<img src=\"img/dl/gradient.gif?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Over- and Underfitting?\n",
    "![under_over](https://miro.medium.com/max/2400/1*JZbxrdzabrT33Yl-LrmShw.png)\n",
    "*image from https://miro.medium.com/max/2400/1*JZbxrdzabrT33Yl-LrmShw.png*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "- In NLP, recurrent neural networks (RNN) are commonly used to analyse sequences. \n",
    "- It takes in a sequence of words, one at a time, and produces hidden states ($h$) after each steps. \n",
    "- RNN-s are used recurrently by feeding in the current word and the hidden state from the previous word.\n",
    "- Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$ (fully connected layer) to reduce the dimension into the dimension of the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment1.png)\n",
    "\n",
    "_(image from bentrevett)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn2](https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn3](https://miro.medium.com/max/770/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM\n",
    "\n",
    "- One of the biggest problem of recurrent neural networks is the vanishing gradient problem. \n",
    "- It happens when the gradient shrinks during bakcpropagarion. \n",
    "- If it becomes very small, the network stops learning. This mostly happen when long sentences are present. \n",
    "- LSTM networks address this problem by having an inner memory cell to remember important information or forget others. \n",
    "- LSTM has a similar flow as a RNN, it processes data and passes information as it propagates forward. \n",
    "- The difference is in the operations within the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![lstm](https://miro.medium.com/max/770/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__LSTM__ consists of:\n",
    "\n",
    "- __Forget gate__\n",
    "    - Decides what information should be kept or thrown away\n",
    "    - Information from the previous hidden state and from the current input\n",
    "\n",
    "![forget](https://miro.medium.com/max/770/1*GjehOa513_BgpDDP6Vkw2Q.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Input gate__\n",
    "    - Decides what information is relevant to add from the current step\n",
    "\n",
    "![input](https://miro.medium.com/max/770/1*TTmYy7Sy8uUXxUXfzmoKbA.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Cell state__\n",
    "\n",
    "![cell](https://miro.medium.com/max/770/1*S0rXIeO_VoUVOyrYHckUWg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Output gate__\n",
    "    - Determines what the next hidden state should be\n",
    "\n",
    "![lstm2](https://miro.medium.com/max/770/1*VOXRGhOShoWWks6ouoDN3Q.gif)\n",
    "\n",
    "_(images from [link](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21))_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a classficiation pipeline - But Neural Networks\n",
    "\n",
    "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.4\n",
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install gensim\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we are going to download the dataset using [torchtext](https://pytorch.org/text/stable/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "from torchtext import data\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Import the needed libraries\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we use [pandas](https://pandas.pydata.org/) to read in the dataset into a DataFrame. This time we are going to use the whole Dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#1-World, 2-Sports, 3-Business, 4-Sci/Tech\n",
    "\n",
    "train_data = pd.read_csv(\"./data/ag_news_csv/train.csv\",quotechar='\"', names=['label', 'title', 'description'])\n",
    "test_data = pd.read_csv(\"./data/ag_news_csv/test.csv\",quotechar='\"', names=['label', 'title', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#1-World, 2-Sports, 3-Business, 4-Sci/Tech\n",
    "\n",
    "train_data.groupby(\"label\").size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning model using [Pytorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to import pytorch and set a fixed random seed number for reproducibility\n",
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We will use the same CountVectorizer that we did in the last lab\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "word_to_ix = vectorizer.fit(train_data.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# First we define some other important parameters for our model\n",
    "VOCAB_SIZE = len(word_to_ix.vocabulary_)\n",
    "NUM_LABELS = 4 \n",
    "\n",
    "# Split the training data, but this time we will have a validation dataset instead of just having\n",
    "# train and test\n",
    "tr_data, val_data = split(train_data, test_size=0.3, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Preparing the data loaders for the validation and the test set\n",
    "# Pytorch operates on it's own datatype which is very similar to numpy's arrays\n",
    "# They are called Torch Tensors: https://pytorch.org/docs/stable/tensors.html\n",
    "# They are optimized for training neural networks\n",
    "\n",
    "tr_data_vecs = torch.FloatTensor(word_to_ix.transform(tr_data.title).toarray())\n",
    "tr_labels = tr_data.label.tolist()\n",
    "\n",
    "val_data_vecs = torch.FloatTensor(word_to_ix.transform(val_data.title).toarray())\n",
    "val_labels = val_data.label.tolist()\n",
    "\n",
    "tr_data_loader = [(sample, label-1) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
    "val_data_loader = [(sample, label-1) for sample, label in zip(val_data_vecs, val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# We then define a BATCH_SIZE for our model\n",
    "# Usually we don't feed the whole dataset into our model at once\n",
    "# For this we have the BATCH_SIZE parameter. Choosing this right can also improve the performance\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# The DataLoader(https://pytorch.org/docs/stable/data.html) class helps us to prepare the training batches \n",
    "# It has a lot of useful parameters, one of the is _shuffle_ which will randomize the training dataset in each epoch\n",
    "# This can also improve the performance of our model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_iterator = DataLoader(tr_data_loader,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            )\n",
    "\n",
    "valid_iterator = DataLoader(val_data_loader,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# We will need to copy our model to CPU or GPU, based on what we are using\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  \n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # Note that we could add more Linear Layers here connected to each other\n",
    "        # Then we would also need to have a HIDDEN_SIZE hyperparameter as an input to our model\n",
    "        # Then, with activation functions between them (e.g. RELU) we could have a \"Deep\" model\n",
    "        # This is just an example for a shallow network\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        # Softmax will provide a probability distribution among the classes\n",
    "        # We can then use this for our loss function\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The INPUT_DIM is the size of our input vectors\n",
    "INPUT_DIM = VOCAB_SIZE\n",
    "# We have 4 classes\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/optim.html\n",
    "import torch.optim as optim\n",
    "\n",
    "# The optimizer will update the weights of our model based on the loss function\n",
    "# This is essential for correct training\n",
    "# The _lr_ parameter is the learning rate, it s\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![NLL](https://ljvmiranda921.github.io/assets/png/cs231n-ann/neg_log_demo.png)\n",
    "\n",
    "_(image from [link](https://ljvmiranda921.github.io))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Copy the model and the loss function to the correct device\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def class_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    # Get the predicted label from the probabilities\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    # Calculate the correct predictions batch-wise\n",
    "    correct = (rounded_preds == y).float()\n",
    "    \n",
    "    # Calculate the accuracy of your model\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the train function\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    # We will calculate loss and accuracy epoch-wise based on average batch accuracy\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # You always need to set your model to training mode\n",
    "    # If you don't set your model to training mode the error won't propagate back to the weights \n",
    "    model.train()\n",
    "    \n",
    "    # We calculate the error on batches so the iterator will return matrices with shape [BATCH_SIZE, VOCAB_SIZE]\n",
    "    for texts, labels in iterator:\n",
    "        # We copy the text and label to the correct device\n",
    "        texts = texts.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # We reset the gradients from the last step, so the loss will be calculated correctly (and not added together)\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        # This runs the forward function on your model (you don't need to call it directly)    \n",
    "        predictions = model(texts)\n",
    "\n",
    "        # Calculate the loss and the accuracy on the predictions (the predictions are log probabilities, remember!)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = class_accuracy(predictions, labels)\n",
    "        \n",
    "        # Propagate the error back on the model (this means changing the initial weights in your model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # We add batch-wise loss to the epoch-wise loss\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# The evaluation is done on the validation dataset\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    # On the validation dataset we don't want training so we need to set the model on evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Also tell Pytorch to not propagate any error backwards in the model\n",
    "    # This is needed when you only want to make predictions and use your model in inference mode!\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # The remaining part is the same with the difference of not using the optimizer to backpropagation\n",
    "        for texts, labels in iterator:\n",
    "            # We copy the text and label to the correct device\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = class_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# This is just for measuring training time!\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the epoch number parameter\n",
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# We loop forward on the epoch number\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model on the training set using the dataloader\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    # And validate your model on the validation set\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # If we find a better model, we save the weights so later we may want to reload it\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
