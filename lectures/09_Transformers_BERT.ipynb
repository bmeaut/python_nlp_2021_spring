{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Natural Language Technologies\n",
    "\n",
    "__Lecture 9, Transformers, BERT__\n",
    "\n",
    "__April 13, 2021__\n",
    "\n",
    "__Judit Ács__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention mechanism\n",
    "\n",
    "Attention:\n",
    "- emphasizes the important part of the input\n",
    "- and de-emphasizes the rest.\n",
    "- Mimics cognitive attention.\n",
    "\n",
    "Method:\n",
    "- It does this by assigning weights to the elements of the input sequence.\n",
    "- The weights depend on the current context in the decoder:\n",
    "    - the current decoder hidden state,\n",
    "    - the previous output.\n",
    "- The source vectors are multiplied by the weights and then summed -> **context vector**\n",
    "- The context vector is used for predicting the next output symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dl/attention_mechanism.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "\n",
    "Recall that we used recurrent neural cells, specifically LSTMs to encode and decode sequences.\n",
    "\n",
    "__Problem 1. No parallelism__\n",
    "\n",
    "LSTMs are recurrent, they rely on their left and right history (horizontal arrows), so the symbols need to be processed in order -> no parallelism.\n",
    "\n",
    "__Problem 2. Long-range dependencies__\n",
    "\n",
    "Long-range dependencies are not infrequent in NLP.\n",
    "\n",
    "\"The **people/person** who called and wanted to rent your house when you go away next year **are/is** from California\" -- Miller & Chomsky 1963\n",
    "\n",
    "LSTMs have a problem capturing these because there are too many backpropagation steps between the symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Introduced in [Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et al., 2017\n",
    "\n",
    "Transformers solve Problem 1 by relying purely on attention instead of recurrence.\n",
    "\n",
    "Not having recurrent connections means that sequence position no longer matters.\n",
    "\n",
    "Recurrence is replaced by **self attention**.\n",
    "\n",
    "Each symbol is encoded the following way:\n",
    "\n",
    "__Step 1__: the encoder 'looks' at the other symbols in the input sequence\n",
    "    - In the example above: the representation of **are/is** depends on **people/person** more than any other word in the sentence, it should receive the highest attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\", embed=True)  # from Illustrated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Step 2__: the context vector is passed through a feed-forward network which is shared across all symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"http://jalammar.github.io/images/t/encoder_with_tensors.png\", embed=True)  # from Illustrated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization is available in the [Tensor2tensor notebook in Google Colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other components\n",
    "\n",
    "__Residual connections__\n",
    "\n",
    "- Also called __skip connections__\n",
    "- The output of a module is added to the input\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{layer}(\\text{input}) + \\text{input}\n",
    "$$\n",
    "\n",
    "__Softmax__\n",
    "\n",
    "- Only used in the decoder\n",
    "- Maps the output vector to a probability distribution\n",
    "    - In other words it tells us how likely each symbol is.\n",
    "\n",
    "## Multiple heads and layers\n",
    "\n",
    "Transformers have a number of additional components summarized in this figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dl/transformer.png\")  # from Vaswani et al. 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch support\n",
    "\n",
    "PyTorch has a `nn.Transformer` class and its encoder and decoder versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 12\n",
    "# num_heads = 5  # embedding_dim must be divisible by the number of heads\n",
    "num_heads = 2\n",
    "hidden_size = 7\n",
    "dropout = 0.2\n",
    "TransformerEncoderLayer(embedding_dim, num_heads, hidden_size, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TransformerEncoderLayer(embedding_dim, num_heads, hidden_size, dropout)\n",
    "TransformerEncoder(layer, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(layer, 2)\n",
    "\n",
    "sequence_len = 9\n",
    "batch_size = 3\n",
    "X = torch.rand((sequence_len, batch_size, embedding_dim))\n",
    "y = encoder(X)\n",
    "y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "\n",
    "Without recurrence word order information is lost.\n",
    "\n",
    "Positional information is important:\n",
    "\n",
    "    John loves Mary.\n",
    "    Mary loves John.\n",
    "\n",
    "Transformers apply positional encoding:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{\\text{pos},2i} = \\sin(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}), \\\\\n",
    "\\text{PE}_{\\text{pos},2i+1} = \\cos(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $d_{\\text{model}}$ is the input dimension to the Transformer, usually the embedding size\n",
    "- $\\text{pos}$ is the position of the symbol in the input sequence i.e. first word, second word etc.\n",
    "- $i$ is the coordinate index in the input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a position encoder in PyTorch.\n",
    "\n",
    "For $\\text{pos}=0$, the sine values are 0, and the cosine values are 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.FloatTensor([0.])\n",
    "torch.cos(t), torch.sin(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large $i$ values, the denominator is close to 10000, so it's again close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a few random values for pos\n",
    "pos = torch.randint(512, size=(10, ))\n",
    "print(pos)\n",
    "# Divide by 10000^2*i/d_model. Make 2*i/d_model close to one (high 2*i values)\n",
    "t = pos / (10000 ** 0.95)\n",
    "\n",
    "torch.cos(t), torch.sin(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate the full grid. There are $\\text{maxlen} \\times d_\\text{model}$ values.\n",
    "\n",
    "__maxlen__ is the maximum position we allow. This has to be predefined.\n",
    "\n",
    "__d_model__ is the size of the input, which is embedding_dim in most cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 20\n",
    "d_model = 12\n",
    "\n",
    "pe = torch.zeros((maxlen, d_model))\n",
    "pe.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__pos__ are the indices of the sequence from 0 to $\\text{maxlen}-1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = torch.arange(maxlen, dtype=torch.float)\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{\\text{pos},2i} = \\sin(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}), \\\\\n",
    "\\text{PE}_{\\text{pos},2i+1} = \\cos(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}),\n",
    "$$\n",
    "\n",
    "Let's define the denominator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divterm = 10000 ** (torch.arange(0, d_model, step=2) / float(d_model))\n",
    "divterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.size(), divterm.size()\n",
    "(pos[:, None] / divterm).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe[:, ::2] = torch.sin(pos[:, None] / divterm)\n",
    "sns.heatmap(pe, cmap='RdBu', center=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe[:, 1::2] = torch.cos(pos[:, None] / divterm)\n",
    "sns.heatmap(pe, cmap='RdBu', center=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining it in a `nn.Module`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# took inspiration from here: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, maxlen=50):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(maxlen, d_model)\n",
    "        pos = torch.arange(maxlen, dtype=torch.float)\n",
    "        divterm = 10000 ** (torch.arange(0, d_model, step=2) / float(d_model))\n",
    "        pe[:, ::2] = torch.sin(pos[:, None] / divterm)\n",
    "        pe[:, 1::2] = torch.cos(pos[:, None] / divterm)\n",
    "        \n",
    "        # Since pe is a constant value not a parameter of the module, we register it as a buffer.\n",
    "        # Buffers are part of the state dictionary of the module along with parameters.\n",
    "        # Docs: https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The input sequence may be shorter than maxlen\n",
    "        seqlen = x.size(0)\n",
    "        \n",
    "        # The middle dimension is the batch size.\n",
    "        # We add it as a dummy dimension.\n",
    "        x = x + self.pe[:seqlen, None, :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 12\n",
    "maxlen = 20\n",
    "batch_size = 7\n",
    "seqlen = 11\n",
    "pos_enc = PositionalEncoding(d_model=d_model, dropout=0., maxlen=maxlen)\n",
    "x = torch.rand(size=(seqlen, batch_size, d_model))\n",
    "x_pe = pos_enc(x)\n",
    "x_pe.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual embeddings\n",
    "\n",
    "In GloVe and Word2vec representations, words have static representations, in other words, the same vector is assigned for every occurrence of the word.\n",
    "But words can have different meaning in different contexts, e.g. the word 'stick':\n",
    "\n",
    "1. Find some dry sticks and we'll make a campfire.\n",
    "2. Let's stick with glove embeddings.\n",
    "\n",
    "![elmo](http://jalammar.github.io/images/elmo-embedding-robin-williams.png)\n",
    "\n",
    "_(Peters et. al., 2018 in the ELMo paper)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELMo\n",
    "\n",
    "**E**mbeddings from **L**anguage **Mo**dels\n",
    "\n",
    "Word representations are functions of the full sentences instead of the word alone.\n",
    "\n",
    "Two bidirectional LSTM layers are linearly combined.\n",
    "\n",
    "[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) by Peters et al., 2018, 6300 citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT\n",
    "\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)\n",
    "by Devlin et al. 2018, 17500 citations\n",
    "\n",
    "[BERTology](https://huggingface.co/transformers/bertology.html) is the nickname for the growing amount of BERT-related research.\n",
    "\n",
    "Trained on two tasks:\n",
    "\n",
    "1. Masked language model:\n",
    "\n",
    "    1. 15% of the <s>tokens</s>wordpieces are selected at the beginning.\n",
    "    2. 80% of those are replaced with `[MASK]`,\n",
    "    3. 10% are replaced with a random token,\n",
    "    4. 10% are kept intact.\n",
    "    \n",
    "2. Next sentence prediction:\n",
    "    - Are sentences A and B consecutive sentences?\n",
    "    - Generate 50-50%.\n",
    "    - Binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dl/bert_embedding.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer layers\n",
    "\n",
    "\n",
    "## Finetuning\n",
    "\n",
    "1. Take a trained BERT model.\n",
    "2. Add a small classification layer on top (typically a 2-layer MLP).\n",
    "3. Train BERT along with the classification layer on an annotated dataset.\n",
    "    - Much smaller than the data BERT was trained on\n",
    "\n",
    "Another option: freeze BERT and train the classification layer only.\n",
    "- Easier training regime.\n",
    "- Smaller memory footprint.\n",
    "- Worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"img/dl/bert_encoding_finetuning.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT pretrained checkpoints\n",
    "\n",
    "### BERT-Base\n",
    "\n",
    "- 12 layers\n",
    "- 12 attention heads per layer\n",
    "- 768 hidden size\n",
    "- 110M parameters\n",
    "\n",
    "### BERT-Large\n",
    "\n",
    "- 24 layers\n",
    "- 16 attention heads per layer\n",
    "- 1024 hidden size\n",
    "- 340M parameters\n",
    "\n",
    "### Cased and uncased\n",
    "\n",
    "Uncased: everything is lowercased. Diacritics are removed.\n",
    "\n",
    "### Multilingual BERT - mBERT\n",
    "\n",
    "104 language version trained on the 100 largest Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT implementations\n",
    "\n",
    "[Original Tensorflow implementation](https://github.com/google-research/bert)\n",
    "\n",
    "[Huggingface Transformers](https://huggingface.co/transformers/)\n",
    "- PyTorch implementation originally for BERT-only\n",
    "- Now it supports dozens of other models\n",
    "- Hundreds of other model checkpoints from the community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT tokenization\n",
    "\n",
    "## WordPiece tokenizer\n",
    "\n",
    "BERT's input **must** be tokenized with BERT's own tokenizer.\n",
    "\n",
    "A middle ground between word and character tokenization.\n",
    "\n",
    "Static vocabulary:\n",
    "- Byte-pair encoding: simple frequency-based tokenization method\n",
    "- Continuation symbols (\\#\\#symbol)\n",
    "- Special tokens: `[CLS]`, `[SEP]`, `[MASK]`, `[UNK]`\n",
    "- It tokenizes everything, falling back to characters and `[UNK]` if necessary\n",
    "\n",
    "`AutoTokenizer` is a factory class for pretrained tokenizers. ng id. `from_pretrained` instantiates the corresponding class and loads the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(type(t))\n",
    "print(len(t.get_vocab()))\n",
    "\n",
    "t.tokenize(\"My beagle's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"Русский\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cased** models keep diacritics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "t.tokenize(\"My beagle's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t.get_vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It character tokenizes Chinese and Japanese but doesn't know all the characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"日本語\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korean is missing from this version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"한 한국어\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mBERT tokenization\n",
    "\n",
    "104 languages, 1 vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"My puppy's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"한 한국어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.tokenize(\"日本語\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BERT\n",
    "\n",
    "## Using `BertModel` directly\n",
    "\n",
    "`AutoModel`\n",
    "- each pretrained checkpoint has a string id. `from_pretrained` instantiates the corresponding class and loads the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModel.from_pretrained('bert-base-cased')\n",
    "type(model), type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"There are black cats and black dogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`__call__` return a dictionary of BERT's encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"There are black cats and black dogs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be used for pairs of sentences. Note the values of `token_type_ids`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"There are black cats and black dogs.\", \"Another sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be used for multiple sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"There are black cats and black dogs.\", \"There are two white cats.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need tensors as inputs for BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer(\"There are black cats and black dogs.\", return_tensors='pt')\n",
    "encoded['input_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded, return_dict=True)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['last_hidden_state'].size(), output['pooler_output'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting all layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoded, output_hidden_states=True, return_dict=True)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(output['hidden_states']), output['hidden_states'][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove variable from the global namespace, run the garbage collector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT applications\n",
    "\n",
    "### Sequence classification\n",
    "\n",
    "Pretrained model for sentiment analysis.\n",
    "\n",
    "Base model: `distilbert-base-uncased`\n",
    "\n",
    "Finetuned on the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) or SST-2, a popular sentiment analysis dataset.\n",
    "\n",
    "Model id: `distilbert-base-uncased-finetuned-sst-2-english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "nlp(\"This is an amazing class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"This is not a good class but it's not too bad either.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"This is not a class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence tagging/labeling: Named entity recognition\n",
    "\n",
    "Base model: `bert-large-cased`\n",
    "\n",
    "Finetuned on [CoNLL-2003 NER](https://www.clips.uantwerpen.be/conll2003/ner/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(\"jupiter is a Planet that orbits around James the center of the Universe\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = nlp(\"George Clooney has a pet pig named Estella.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"translation_en_to_fr\")\n",
    "print(nlp(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the [blessé - blessed false cognate](https://frenchtogether.com/french-english-false-friends/) is handled correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"I was blessed by God after I injured my head.\", max_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked language modeling\n",
    "\n",
    "Uses `distilroberta-base`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"fill-mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Twitter is a bad idea /s> [MASK]\"\n",
    "\n",
    "for n in range(10):\n",
    "    result = nlp(f\"{prompt} {nlp.tokenizer.mask_token}\")\n",
    "    token = result[0]['token_str'][1:]\n",
    "    prompt += \" \" + token\n",
    "    \n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pred = nlp(f\"HuggingFace is creating a {nlp.tokenizer.mask_token} that the community uses to solve NLP tasks.\")\n",
    "pprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nlp(f\"{nlp.tokenizer.mask_token} is a very good idea.\")\n",
    "pprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nlp(f\"{nlp.tokenizer.mask_token} is a bad idea.\")\n",
    "pprint(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other models\n",
    "\n",
    "## Pretrained models\n",
    "\n",
    "RoBERTa: identical model, larger training data, different training objective\n",
    "\n",
    "DistilBERT: smaller version of BERT. It was _distilled_ or compressed from BERT with a student-teacher setup.\n",
    "\n",
    "ALBERT: smaller BERT\n",
    "\n",
    "XLM-RoBERTa: multilingual version of RoBERTa\n",
    "\n",
    "Distil-mBERT: distilled multilingual BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Community models\n",
    "\n",
    "[Over 1000 community contributions](https://huggingface.co/models)\n",
    "\n",
    "## huBERT\n",
    "\n",
    "The first Hungarian-only model and the only one registered on Huggingface.\n",
    "Other models are available at https://hilanco.github.io/.\n",
    "\n",
    "BERT base, trained on Webcorpus 2.0, a version of CommonCrawl.\n",
    "\n",
    "Its tokenizer works much better for Hungarian than mBERT's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hubert_tokenizer = AutoTokenizer.from_pretrained('SZTAKI-HLT/hubert-base-cc')\n",
    "# hubert = AutoModel.from_pretrained('SZTAKI-HLT/hubert-base-cc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = (\"George Clooney Magyarországról szóló, az Orbán-kormányt kritizáló levelére miniszteri és \"\n",
    "        \"államtitkári szinten is reagált a magyar kormány.\")\n",
    "hubert_tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 text generation\n",
    "\n",
    "Causal language modeliing is when the $i^{th}$ token is modeled based on all the previous tokens as opposed to masked language modeling where both left and right context are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_generator(\"This is a serious issue we should address\", max_length=50, do_sample=False)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_generator(\"Twitter is a bad idea, Jack Dorsey had a bad day when he came up with it\", max_length=100, do_sample=False)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del text_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further information\n",
    "\n",
    "[Official PyTorch Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "\n",
    "[Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- Famous blog post with a detailed gentle introduction to Transformers\n",
    "\n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- A walkthrough of original Transformer paper with code and detailed illustration\n",
    "\n",
    "[Huggingface Transformers - Summary of tasks](https://huggingface.co/transformers/task_summary.html)\n",
    "\n",
    "[My blog post about mBERT's tokenizer](http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
