{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python and Natural Language Technologies\n",
    "\n",
    "__Lecture 8, Sequence modeling__\n",
    "\n",
    "__March 30, 2021__\n",
    "\n",
    "__Judit Ács__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "1. Overview of sequence modeling\n",
    "    - Sequence elements\n",
    "    - Types of models\n",
    "    - Some applications\n",
    "2. A bare bone sequence classification example\n",
    "    - Cover the details of modeling and training\n",
    "    - Data preparation with Pandas\n",
    "    - Training on a small task that runs quickly on a laptop CPU \n",
    "    - Use advanced Python features instead of `torchtext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard `torch` imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence elements\n",
    "\n",
    "We deal with sequences in NLP:\n",
    "- a token is a sequence of characters/morphemes\n",
    "- a sentence is a sequence of tokens\n",
    "- a paragraph is a sequence of sentences\n",
    "- a dialogue is a sequence of utterances\n",
    "- etc.\n",
    "\n",
    "What are the elements of these sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words\n",
    "\n",
    "Pros:\n",
    "\n",
    "- More or less well-defined in most languages\n",
    "- Relatively short sequences (a sentence is rarely longer than 30 tokens)\n",
    "\n",
    "Cons:\n",
    "- Difficult tokenization in some languages\n",
    "- Large vocabulary (100,000+ easily)\n",
    "- Out-of-vocabulary words are always there regardless of the size of the vocabulary\n",
    "- Many rare words\n",
    "    - Hapax: a word that only appears once in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters\n",
    "\n",
    "Pros:\n",
    "- Smaller vocabulary although logographic writing systems (Chinese and Japanese) have thousands of characters\n",
    "- Easy tokenization\n",
    "- Well defined: Unicode symbols\n",
    "\n",
    "Cons:\n",
    "- Long sequences\n",
    "- Too fine-grained, token level information is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subwords\n",
    "\n",
    "- Multiple characters but smaller than words\n",
    "- Modern language models use subword vocabularies\n",
    "- We will cover these next week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification\n",
    "\n",
    "Assign a single label to the full sequence:\n",
    "\n",
    "<img src=\"img/tikz/abstract_sequence_classification.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Applications__\n",
    "\n",
    "- Topic classification (AG News dataset from Lab 7)\n",
    "- Sentiment analysis: is this sentence or paragraph a positive (1) or a negative (0) review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tikz/example_sequence_classification.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence tagging\n",
    "\n",
    "Assign a label to each element of the sequence:\n",
    "\n",
    "<img src=\"img/tikz/abstract_sequence_tagging.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Applications__\n",
    "\n",
    "- part-of-speech tagging\n",
    "- named entity recognition (NER)\n",
    "\n",
    "<img src=\"img/tikz/example_sequence_tagging.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq\n",
    "\n",
    "<img src=\"img/tikz/abstract_seq2seq.png\" width=600px>\n",
    "\n",
    "- Maps a source sequence to a target sequence\n",
    "    - Arbitrary length\n",
    "    \n",
    "- Two steps:\n",
    "    1. Encode: create a representation of the source\n",
    "    2. Decode: generate the target representation\n",
    "        - autoregressive: generate tokens from left-to-right one-by-one (condition on the left context)\n",
    "        \n",
    "- Usually implemented as two separate neural networks for example:\n",
    "    - The encoder is a bidirectional LSTM\n",
    "    - The decoder is a unidirectional LSTM\n",
    "    \n",
    "- Applications:\n",
    "    - Neural machine translation\n",
    "    - Morphological inflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "Seq2seq performs poorly in its naive form since the decoder has to generate the whole output based on a single hidden vector that represents the full input sequence.\n",
    "\n",
    "**Attention** gives peak into the input sequence [image source](https://aihub.cloud.google.com/u/0/p/products%2F024b89fd-9bc8-4c24-b8a8-e347479f3270):\n",
    "    \n",
    "<img src=\"img/dl/attention_mechanism.jpg\" width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other tasks\n",
    "\n",
    "There are many other NLP tasks that are solved with some combination of the above models.\n",
    "\n",
    "- Sentence pair classification: the same as sequence classification except we assign a label to a __pair__ of sentences\n",
    "    - paraphrase identification\n",
    "- Span level tasks\n",
    "- Tree-based tasks\n",
    "    - Universal Dependencies (Lecture 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence classification example\n",
    "\n",
    "We will now train a sequence classification model on Hungarian morphology. This model can be trained on a laptop CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "\n",
    "Neural networks are usually trained with the [backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm.\n",
    "\n",
    "The data flow in neural networks implicitly defines a **computation graph**, this is called **forward pass**.\n",
    "\n",
    "The output is compared against the ground truth or label and the difference or cost is quantified by **loss function** also called **cost function**.\n",
    "\n",
    "If the loss function is differentiable with respect to the parameters, we can compute the gradient w.r.t. to all parameters i.e. we can quantify 'how responsible' a parameter is for the loss using the chain rule. This is called **backpropagation**.\n",
    "\n",
    "An optimizer then updates the parameters. The update is proportional to the gradient. This is called **gradient descent**.\n",
    "\n",
    "More information: [Backpropagation chapter](https://www.deeplearningbook.org/contents/mlp.html#pf25) from [Deep Learning](https://www.deeplearningbook.org/) by Goodfellow, Bengio and Courville\n",
    "\n",
    "We will now discuss the building blocks of the sequence classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Embedding`\n",
    "\n",
    "`nn.Embedding` maps integers to continuous vectors. Its mandatory parameters are:\n",
    "- `num_embeddings`: the size of the vocabulary\n",
    "- `embedding_dim`: the size of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(5, 3)\n",
    "embedding(torch.LongTensor([1, 4]))\n",
    "# embedding(torch.LongTensor([1, 5]))  # raises IndexError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works with higher order tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(\n",
    "    [[1, 2], [0, 1]]\n",
    ")\n",
    "emb = embedding(input_tensor)\n",
    "print(f\"{input_tensor.size() = }\")\n",
    "print(f\"{emb.size() = }\")\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(\n",
    "    [[[1, 2], [0, 1]]]\n",
    ")\n",
    "emb = embedding(input_tensor)\n",
    "print(f\"{input_tensor.size() = }\")\n",
    "print(f\"{emb.size() = }\")\n",
    "emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding's parameters can be listed with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pname, param in embedding.named_parameters():\n",
    "    print(pname, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.LSTM`\n",
    "\n",
    "LSTM is the most popular recurrent cell that takes a sequence as an input and processes it one by one while updating its hidden state. Its parameters are (from [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM)):\n",
    "- `input_size`: The number of expected features in the input x\n",
    "- `hidden_size`: The number of features in the hidden state h\n",
    "- `num_layers`: Number of recurrent layers. E.g., setting num_layers=2 would mean stacking two LSTMs together to form a stacked LSTM, with the second LSTM taking in outputs of the first LSTM and computing the final results. Default: 1\n",
    "- `bias`: If False, then the layer does not use bias weights b_ih and b_hh. Default: True\n",
    "- `batch_first`: If True, then the input and output tensors are provided as (batch, seq, feature). Default: False\n",
    "- `dropout`: If non-zero, introduces a Dropout layer on the outputs of each LSTM layer except the last layer, with dropout probability equal to dropout. Default: 0\n",
    "- `bidirectional`: If True, becomes a bidirectional LSTM. Default: False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(5, 12, num_layers=1, bidirectional=False, batch_first=True, dropout=0)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pname, param in lstm.named_parameters():\n",
    "    print(pname, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its input **must** be 3D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.rand((3, 2))\n",
    "# lstm(input_tensor)  # raises RunTimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# batch_size X sequence_length X input_size\n",
    "# 3 X 2 X 5\n",
    "input_tensor = torch.rand((3, 2, 5))\n",
    "outputs, (h, c) = lstm(input_tensor)\n",
    "\n",
    "print(f\"{outputs.size() = }\")\n",
    "print(f\"{h.size() = }\")\n",
    "print(f\"{c.size() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice we generally use bidirectional LSTMs, which are implemented as two unidirectional LSTMs.\n",
    "\n",
    "`h` and `c` are now the final states of both unidirectional LSTMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(5, 12, num_layers=1, bidirectional=True, batch_first=True, dropout=0)\n",
    "\n",
    "input_tensor = torch.rand((3, 2, 5))\n",
    "outputs, (h, c) = lstm(input_tensor)\n",
    "\n",
    "print(f\"{outputs.size() = }\")\n",
    "print(f\"{h.size() = }\")\n",
    "print(f\"{c.size() = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(5, 12, num_layers=3, bidirectional=True, batch_first=True, dropout=0)\n",
    "\n",
    "input_tensor = torch.rand((3, 2, 5))\n",
    "outputs, (h, c) = lstm(input_tensor)\n",
    "\n",
    "print(f\"{outputs.size() = }\")\n",
    "print(f\"{h.size() = }\")\n",
    "print(f\"{c.size() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining `nn.LSTM` with `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model parameters\n",
    "vocab_size = 10\n",
    "embedding_size = 5\n",
    "lstm_hidden_size = 6\n",
    "\n",
    "embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "lstm = nn.LSTM(embedding_size, lstm_hidden_size, num_layers=1, bidirectional=True, batch_first=True, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# input parameters (not that these are independent of the model)\n",
    "batch_size = 3\n",
    "sequence_length = 4\n",
    "\n",
    "input_ids = torch.randint(vocab_size, (batch_size, sequence_length))\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedded = embedding(input_ids)\n",
    "outputs, (h, c) = lstm(embedded)\n",
    "\n",
    "print(f\"{input_ids.size() = }\")\n",
    "print(f\"{embedded.size() = }\")\n",
    "print(f\"{outputs.size() = }\")\n",
    "print(f\"{h.size() = }\")\n",
    "print(f\"{c.size() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `nn.Linear`\n",
    "\n",
    "`nn.Linear` implements a matrix projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dense = nn.Linear(3, 5)\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pname, param in dense.named_parameters():\n",
    "    print(pname, param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.rand((2, 3))\n",
    "output = dense(input_tensor)\n",
    "print(f\"{output.size() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner dimensions must match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.rand((2, 4))\n",
    "# output = dense(input_tensor)  # raises RuntimeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `LSTMClassifier` class\n",
    "\n",
    "We can now define our own LSTM sequence classifier model.\n",
    "\n",
    "All PyTorch modules must subclass `nn.Module` (or one of its subclasses) and call `init` before any attribute assignment.\n",
    "\n",
    "There two methods we have to implement:\n",
    "- `__init__`: defines submodules. These constitute the nodes _computation graph_.\n",
    "- `forward` implements the forward pass of the module. This is how we map the input to the output. The way we pass the input through the module implicitly builds a directed graph of the submodules named _computation graph_.\n",
    "\n",
    "The backward pass is automatically handled by PyTorch but it can be overriden by implementind the `backward` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dense = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    def forward(self, sequences):\n",
    "        # sequences: batch_size X sequence_length\n",
    "        embedded = self.embedding(sequences)\n",
    "        \n",
    "        # lstm_outputs: batch_size X sequence_length X 2*hidden_size\n",
    "        # h: 2 X batch_size X hidden_size\n",
    "        # c: 2 X batch_size X hidden_size\n",
    "        lstm_outputs, (h, c) = self.lstm(embedded)\n",
    "        \n",
    "        # h: batch_size X 2*hidden_size\n",
    "        h = torch.cat((h[0], h[1]), dim=-1)\n",
    "        \n",
    "        # output: batch_size X output_size\n",
    "        output = self.dense(h)\n",
    "        return output\n",
    "        \n",
    "        \n",
    "toy_classifier = LSTMClassifier(3, 10, 5, 2)\n",
    "toy_input = torch.LongTensor([\n",
    "    [0, 1, 0, 2],\n",
    "    [0, 1, 0, 2],\n",
    "    [0, 0, 0, 2],\n",
    "    [1, 1, 2, 0],\n",
    "])\n",
    "toy_classifier(toy_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Under the hood\n",
    "\n",
    "PyTorch registers every attribute in `__init__` that is an instance of `nn.Module` in the parameters of the module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for pname, param in toy_classifier.named_parameters():\n",
    "    print(pname, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **not** true for lists and other complex data types. PyTorch does not attempt to traverse them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(12, 4), nn.Linear(15, 9)]\n",
    "        \n",
    "for pname, param in SimpleModule().named_parameters():\n",
    "    print(pname, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is `nn.ModuleList` or `nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(12, 4), nn.Linear(15, 9)])\n",
    "        \n",
    "for param in SimpleModule().named_parameters():\n",
    "    print(f\"name: {param[0]}, size: {param[1].size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Sequential` chains multiple modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SimpleModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(12, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(15, 9),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        out = self.layers(out)\n",
    "        return out\n",
    "    \n",
    "        \n",
    "for pname, param in SimpleModule().named_parameters():\n",
    "    print(pname, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "We will now train a small classifier that predicts the case of Hungarian nouns. Hungarian has a rich case system in place of prefixes used by English. Some examples are:\n",
    "\n",
    "| Case | Hungarian | English |\n",
    "| ---- | ---- | ---- |\n",
    "| Nominative | ház | house |\n",
    "| Instrumental | házzal | with (a) house |\n",
    "| Ablative  | háztól | from at (a) house |\n",
    "| Elativus  | házból | from inside (a) house |\n",
    "\n",
    "\n",
    "We will train a character-level model that predicts the case based on the word form. This is an easy task since most of the time the grammatical case is obvious from the last 3 characters of the word.\n",
    "\n",
    "Our model looks like this:\n",
    "\n",
    "<img src=\"img/tikz/hungarian_case_lstm.png\" width=600px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_table(\"data/unimorph/hun_train.tsv\")\n",
    "dev_df = pd.read_table(\"data/unimorph/hun_dev.tsv\")\n",
    "test_df = pd.read_table(\"data/unimorph/hun_test.tsv\")\n",
    "print(len(train_df), len(dev_df), len(test_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling\n",
    "\n",
    "We don't need much data to train the model, let's downsample it and train on a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(1000, random_state=1).reset_index(drop=True)\n",
    "dev_df = dev_df.sample(200, random_state=1).reset_index(drop=True)\n",
    "test_df = test_df.sample(200, random_state=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the vocabulary\n",
    "\n",
    "We need to map each character to an integer id. For this we need to define a `char->int` mapping that is as big as the alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphabet = set()\n",
    "for token in train_df.infl:\n",
    "    alphabet |= set(token)\n",
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and some extra symbols:\n",
    "\n",
    "1. PAD: used as filler symbols for shorter sequences (see later)\n",
    "2. BOS: beginning-of-sequence. Indicates the start of the sequence.\n",
    "2. EOS: end-of-sequence. Indicates the end of the sequence.\n",
    "2. UNK: unknown. Symbols that fall out of the vocabulary are replaced with this symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alphabet.add('<PAD>')\n",
    "alphabet.add('<BOS>')\n",
    "alphabet.add('<EOS>')\n",
    "alphabet.add('<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab = {symbol: i for i, symbol in enumerate(alphabet)}\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_token(token):\n",
    "    ids = []\n",
    "    ids.append(vocab['<BOS>'])\n",
    "    # dev and test might contain characters outside the alphabet\n",
    "    ids.extend(vocab.get(c, vocab['<UNK>']) for c in token)\n",
    "    ids.append(vocab['<EOS>'])\n",
    "    return ids\n",
    "\n",
    "print(f\"{encode_token('alma') = }\")\n",
    "print(f\"{vocab['<UNK>'] = }\")\n",
    "print(f\"{encode_token('ALMA') = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['encoded'] = train_df.infl.apply(encode_token)\n",
    "dev_df['encoded'] = dev_df.infl.apply(encode_token)\n",
    "test_df['encoded'] = test_df.infl.apply(encode_token)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "The input sequences different in length at the moment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=train_df.infl.str.len(), palette='tab10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We append **PAD** symbols to the shorter sequences like this:\n",
    "\n",
    "<img src=\"img/tikz/padding.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxlen = train_df.encoded.apply(len).max()\n",
    "print(maxlen)\n",
    "\n",
    "def pad_sequence(sequence):\n",
    "    if len(sequence) > maxlen:\n",
    "        return sequence[:maxlen]\n",
    "    return sequence + [vocab['<PAD>'] for _ in range(maxlen-len(sequence))]\n",
    "\n",
    "print(pad_sequence([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['padded'] = train_df.encoded.apply(pad_sequence)\n",
    "dev_df['padded'] = dev_df.encoded.apply(pad_sequence)\n",
    "test_df['padded'] = test_df.encoded.apply(pad_sequence)\n",
    "\n",
    "train_df['padded'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need the original lengths of each sequence later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['seqlen'] = train_df.encoded.apply(len)\n",
    "dev_df['seqlen'] = dev_df.encoded.apply(len)\n",
    "test_df['seqlen'] = test_df.encoded.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing labels\n",
    "\n",
    "There are 18 labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_to_id = {label: i for i, label in enumerate(train_df.case.unique())}\n",
    "label_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add a `label` column to each dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df['label'] = train_df.case.apply(lambda c: label_to_id[c])\n",
    "dev_df['label'] = dev_df.case.apply(lambda c: label_to_id[c])\n",
    "test_df['label'] = test_df.case.apply(lambda c: label_to_id[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract input and output tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(np.array(list(train_df.padded)))\n",
    "y_train = torch.LongTensor(train_df.label.values)\n",
    "seqlen_train = torch.LongTensor(train_df.seqlen.values)\n",
    "print(f\"{X_train.size() = },\\n{y_train.size() = }\\n{seqlen_train.size() = }\\n\")\n",
    "\n",
    "X_dev = torch.from_numpy(np.array(list(dev_df.padded)))\n",
    "y_dev = torch.LongTensor(dev_df.label.values)\n",
    "seqlen_dev = torch.LongTensor(dev_df.seqlen.values)\n",
    "print(f\"{X_dev.size() = },\\n{y_dev.size() = }\\n{seqlen_dev.size() = }\\n\")\n",
    "\n",
    "X_test = torch.from_numpy(np.array(list(test_df.padded)))\n",
    "y_test = torch.LongTensor(test_df.label.values)\n",
    "seqlen_test = torch.LongTensor(test_df.seqlen.values)\n",
    "print(f\"{X_test.size() = },\\n{y_test.size() = }\\n{seqlen_test.size() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PackedSequence`\n",
    "\n",
    "We need to modify `LSTMTagger` to support padding. The last output is now different for each sequence:\n",
    "\n",
    "<img src=\"img/tikz/padding_last_highlight.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.dense = nn.Linear(hidden_size * 2, output_size)\n",
    "        \n",
    "    # the input signature of forward changes\n",
    "    def forward(self, sequences, sequence_lens):\n",
    "        embedded = self.embedding(sequences)\n",
    "        \n",
    "        # THIS IS THE MODIFIED PART\n",
    "        # returns a PackedSequence object\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded,\n",
    "            sequence_lens,\n",
    "            enforce_sorted=False,\n",
    "            batch_first=True)\n",
    "        packed_outputs, (h, c) = self.lstm(packed)\n",
    "        # extract LSTM outputs (not used here)\n",
    "        lstm_outputs, lens = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        \n",
    "        h = torch.cat((h[0], h[1]), dim=-1)\n",
    "        output = self.dense(h)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model\n",
    "\n",
    "The input and the output size are determined by the alphabet and the number of labels, the rest are up to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size = len(vocab)\n",
    "embedding_size = 30\n",
    "hidden_size = 64\n",
    "output_size = train_df.label.nunique()\n",
    "\n",
    "model = LSTMClassifier(input_size, embedding_size, hidden_size, output_size)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "Most datasets do not fit into the GPU memory so we process them in smaller chunks called _batches_.\n",
    "\n",
    "There are many solutions for batching but it can also be implemented with simple class.\n",
    "\n",
    "Note that the init function takes an arbitrary number of positional arguments (`*tensors`) and one mandatory keyword (`batch_size`). This class is a simplified version of batching, it lacks many features such as shuffling or sorting by sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BatchedIterator:\n",
    "    def __init__(self, *tensors, batch_size):\n",
    "        # all tensors must have the same first dimension\n",
    "        assert len(set(len(tensor) for tensor in tensors)) == 1\n",
    "        self.tensors = tensors\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def iterate_once(self):\n",
    "        num_data = len(self.tensors[0])\n",
    "        for start in range(0, num_data, self.batch_size):\n",
    "            end = start + self.batch_size\n",
    "            yield tuple(tensor[start:end] for tensor in self.tensors)\n",
    "            \n",
    "            \n",
    "print(\"Two tensors:\")\n",
    "for batch in BatchedIterator([1, 2, 3], [2, 1, 2], batch_size=2).iterate_once():\n",
    "    print(batch)\n",
    "    \n",
    "print(\"\\nOne tensor:\")\n",
    "for batch in BatchedIterator([2, 1, 2], batch_size=2).iterate_once():\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_iter = BatchedIterator(X_train, seqlen_train, y_train, batch_size=501)\n",
    "for X, seqlens, y in train_iter.iterate_once():\n",
    "    print(f\"{X.size() = }, {seqlens.size() = }, {y.size() = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and optimizer\n",
    "\n",
    "The **loss function** or **cost function** quantifies cost of the model output differing from the expected target values.\n",
    "\n",
    "The optimizer adjusts the model's parameters in accordance with the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check\n",
    "\n",
    "Train and dev accuracy should be really bad without training.\n",
    "\n",
    "Do **NOT** touch the test data while development finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits = model(X_train, seqlen_train)\n",
    "y = logits.argmax(axis=1)\n",
    "accuracy = torch.sum(torch.eq(y, y_train)) / y.size(0)\n",
    "print(f\"Train accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits = model(X_dev, seqlen_dev)\n",
    "y = logits.argmax(axis=1)\n",
    "accuracy = torch.sum(torch.eq(y, y_dev)) / y.size(0)\n",
    "print(f\"Dev accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We collect training statistics at the end of each epochs in `metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "metrics = defaultdict(list)\n",
    "train_iter = BatchedIterator(X_train, seqlen_train, y_train, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model batch by batch and then evaluate it on the train and the dev data at the end of each epoch.\n",
    "Since the dataset is small, we can evaluate it the whole data in one step without batching.\n",
    "\n",
    "Note that the model should be set to **train** or **eval** mode accordingly. Stochastic steps such as dropout are disabled in **eval** mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # Training loop\n",
    "    for X_batch, seqlen_batch, y_batch in train_iter.iterate_once():\n",
    "        y_out = model(X_batch, seqlen_batch)\n",
    "        loss = criterion(y_out, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()  # or model.train(False)\n",
    "    # Train and dev loss at the end of the epoch\n",
    "    y_out = model(X_train, seqlen_train)\n",
    "    train_loss = criterion(y_out, y_train).item()\n",
    "    metrics['train_loss'].append(train_loss)\n",
    "    labels = y_out.argmax(axis=1)\n",
    "    train_accuracy = (torch.eq(y_train, labels).sum() / labels.size(0)).item()\n",
    "    metrics['train_accuracy'].append(train_accuracy)\n",
    "    \n",
    "    y_out = model(X_dev, seqlen_dev)\n",
    "    dev_loss = criterion(y_out, y_dev).item()\n",
    "    metrics['dev_loss'].append(dev_loss)\n",
    "    labels = y_out.argmax(axis=1)\n",
    "    dev_accuracy = (torch.eq(y_dev, labels).sum() / labels.size(0)).item()\n",
    "    metrics['dev_accuracy'].append(dev_accuracy)\n",
    "    \n",
    "    print(f\"{epoch=} -- {train_loss=:.3f} - {train_accuracy=:.1%} - {dev_loss=:.3f} - {dev_accuracy=:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "sns.lineplot(data=metrics['train_loss'], ax=ax[0], label='train loss')\n",
    "sns.lineplot(data=metrics['dev_loss'], ax=ax[0], label='dev loss')\n",
    "\n",
    "sns.lineplot(data=metrics['train_accuracy'], ax=ax[1], label='train acc')\n",
    "sns.lineplot(data=metrics['dev_accuracy'], ax=ax[1], label='dev acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logits = model(X_test, seqlen_test)\n",
    "test_prediction = logits.argmax(axis=1)\n",
    "test_accuracy = torch.sum(torch.eq(test_prediction, y_test)) / y_test.size(0)\n",
    "print(f\"Test accuracy: {test_accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorrectly classified examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df['prediction'] = test_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recovering labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "test_df['predicted_case'] = test_df['prediction'].apply(lambda id_: id_to_label[id_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df[test_df.prediction != test_df.label][['infl', 'case', 'predicted_case']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further topics\n",
    "\n",
    "## Early stopping\n",
    "\n",
    "Stop the training process if the development metrics no longer improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout disables a random subset of neurons during each training step. It's generally set to 10-20%. Dropout usually improves generalization.\n",
    "\n",
    "It should be disabled in evaluation steps and during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Fit the train set very closely but lose generalization.\n",
    "\n",
    "[Image source](https://www.quora.com/What-are-the-key-trade-offs-between-overfitting-and-underfitting)\n",
    "\n",
    "<img src=\"img/dl/overfitting.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the GPU\n",
    "\n",
    "Moving things manually to the GPU:\n",
    "- model: move once\n",
    "- criterion: move once\n",
    "- data: move one batch at a time\n",
    "\n",
    "This should be automatically handled by your code the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    tagger = tagger.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models\n",
    "\n",
    "All `nn.Modules` have a `state_dict` attribute, a dictionary of their parameters. This can be partially or fully saved with `torch.save` and loaded by `torch.load`.\n",
    "\n",
    "(Official tutorial)[https://pytorch.org/tutorials/beginner/saving_loading_models.html]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "**Inference** is when we use the model for prediction and do not train it.\n",
    "\n",
    "The models should be set to `eval` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
